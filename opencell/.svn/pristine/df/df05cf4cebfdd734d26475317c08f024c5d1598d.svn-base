\documentclass{sig-alternate}
\usepackage{url}
\usepackage{framed}
\usepackage{amsmath}
\usepackage[ruled]{algorithm2e}
\usepackage{graphicx}
\usepackage{comment}
\begin{document}

\title{OpenCell: Open KPIs for Ajusted Bit Rate Video Streaming}
\numberofauthors{4} 
\author{%
		\alignauthor X. Kelvin Zou\\
		\affaddr{Princeton University}\\
		 \email{xuanz@princeton.edu}
		\and
		\alignauthor Xin Jin\\
       		 \affaddr{Princeton University}\\
        		\email{xinjin@princeton.edu}  
		\and
      		 \alignauthor Rittwik Jana\\
      		 \affaddr{AT\&T Lab}\\
       		\email{rjana@research.att.com}
		\and
        		\alignauthor Jennifer Rexford\\
      		 \affaddr{Princeton University}\\
      		 \email{jrx@princeton.edu} 
}
\maketitle

%\begin{abstract}
%...
%\end{abstract}

\section{Motivation and design}
It has been a growing demand for on-demand video streaming service along with the growth of 4G LTE deployment. But it has been a challenging problem for content providers to offer a stable and efficient video streaming service. Most of the video optimization are designed for wireline networks and do not apply to mobile networks. The wireless network has its unique characteristics that are distinguishable from the wireline network. The cellular network links are more dynamic due to user mobility and signal interference, some behaviors such as handoff also only occurs in the cellular networks. In addition, the cellular networks have instrumented many middleboxes for flow pacing and enforcing quality of service, which severely affects end-to-end probing -- a mainstream network monitoring approach for most content providers. The physical and architectural distinctions between wireline and mobile network ask us for a different optimization scheme for video streaming. 

Given that monitoring the wireless network condition is quite challenging at the end-to-end side, we need to explore other options. Previous studies have shown that the last mile has long been the bottleneck for cellular network\cite{LASTMILE}, this means that ISPs can characterize the mobile network conditions via monitoring the last hop wireless channel at basestations. The special feature of mobile network essentially creates a tale of two cities: on the one hand mobile network service providers are capable of knowing users' network conditions clearly, on the other hand the content providers are often confused by sporadic cellular network behaviors. This brings us an unique opportunities for a collaborative model between ISPs and content providers: ISPs can monitor the network conditions and expose it to content providers to facilitate their decisions in terms of video streaming sessions. 

In this paper we propose a new design of video streaming with the help of exposed knowledge about network conditions, in particular available throughput and handoff information. The available throughput, in practice can be computed via the load on each cell, the number of users in the cell and the link quality for each user. The available can be predicted or enforced through scheduler. Handoff are usually initialized by UEs, and are handled by MME(Mobility Management Entity) and UE together. We show that if we have the knowledge, the video play session will ...
Outline of the project:
\begin{itemize}
\item Conduct a measurement study to show the highly dynamic wireless links in cellular network and dynamic utilization of eNodeBs, and reveal the opportunities of improving video streaming service. 
%\item Apply machine learning techniques to predict the future link quality and maximum achieveable bandwidth for each user, and convert it to KPIs(Key Performance Indicators). 
%\item Design a scalable architecture to make predictions, compute KPIs and expose them to video streaming providers on the fly.
\item Design an algorithm to show that by using the KPIs we have significant improvement in video streaming service.
\end{itemize}

\section{Background}\label{background}
\subsection{Video Streaming}
\emph{Chunk-based Download and Play:} most video streaming service providers including Netflix and YouTube are downloading and playing in chunks. A video is divided into hundreds to thousands of chunks, and each chunk is several seconds long. The download has an on-off behavior\cite{OnOff}: e.g., the client keeps moving the download point based on play progress, the download can be ahead of play progress with a relatively constant time window (usually at minute level), or dynamic window size(e.g., for YouTube it is $0.25 * PlayTime$) so the video play sessions can be resilient to some network congestion and temporary low throughput. It downloads the whole chunk when the chunk falls in the download sliding window, and cannot play the chunk until the chunk is fully downloaded. 

\emph{Dynamic Play Bit Rate Selection:} each video chunk is encoded in several bit rates, a higher bit rate means a better quality. Video streaming applications select a certain bit rate for each chunk to download, based on some utility function of both efficiency and stability. A video play session can consist of chunks with different bit rates and the video player will stitch the chunks together seamlessly. 

A video streaming application essentially needs to make two decisions: when and which play bit rate to download and play. 


\subsection{LTE Network}

\section{Design}
In this section, we explain our design choice in terms of decision making in section \ref{designchoice}, we then introduce performance metrics in section \ref{metrics} and explain the our architecture in section\ref{architecture}. 

\subsection{Who decides the video streaming?}\label{designchoice}
For video streaming service decision making, there are three different models: \emph{In-network}, \emph{strictly end-to-end} and \emph{End-to-end with in-network knowledge}. 

\emph{In-network techniques} make decisions for users: it selects the video bit rate and schedule the download for users based on its knowledge of the network condition. However in-network approaches lack the insight of the video play progress, e.g., video buffer occupancy since it does not deal with any video decoding. Aside from technical reasons, here are several pricacy/policy reasons for not doing this: most content providers are using encrypted http connections which ISPs cannot access; users may be concerned about their data budgets and thus not choose high bit rate; or content providers have different video play bit rate policies for different users based on their subsrciption contracts. 

\emph{End-to-end techniques} probe the network condition based on round trip time (RTT) and historical throughput, however existence of the proxy makes the RTT very inaccurate as it splits tcp connections. Many pacing instruments in the network also creates unpredictable dynamics between TCP senders and receivers. All existing approaches are measuring the throughput at application layer, however it may be affected by many cross-layer interactions and fail to provide useful insight. In addition, the dynamic nature of the wireless link makes histroical data unable to capture the link behavior, e.g. the available bandwidth may plummit due to fading effects or handoff and recovers very soon after, current end-to-end approaches fail to react to those scenarios in most cases.


\emph{End-to-end with in-network knowledg} model addresses the above issues: the network does not make the decision for the client; but instead helps video streaming service make better decisions via offering some KPIs about the network condition, such as available throughput and handoff. 


\begin{table*}
\begin{tabular} {|c |c |}
\hline
Notation&Meaning\\ \hline
$T$ &the number of time segments\\ \hline
$M$ &the number of video quality levels (encoding bit rates)\\ \hline
$R_i$& the rate for the video with $i^{th}$ highest encoding bit rate\\ \hline
$x_i^t$&indicator of encoding of $R_i$ video play rate at time t, used in offline algorithm \\ \hline

$E$& utilization of used bandwidth over maximum available bandwidth \\ \hline
$\phi^t$ &the number of quality jumps between two consecutive chunks at time t\\ \hline
$D^t$ &current download rate at time t\\ \hline
$H$ &the number of time segments with predicted bandwidth\\ \hline
$z $ &the maximum buffer size (the number of video chunks) \\ \hline
$W $ &the unfilled buffer size (the number of video chunks)\\ \hline
$x_i^{t,l}$&indicator for $l^{th}$ segment with $R_i$ at time t, used in online algorithm \\\hline
$\tilde{E}$&avergae efficiency, used in online algorithm \\\hline
$\tilde{\phi}$ &average fluctuation, used in online algorithm \\\hline


%0Xhline{3\arrayrulewidth}
\end{tabular}
\centering
\caption{Variables for Optimization}
%\floatfoot{Before the bold horizontal line are notations for pro}
\end{table*}


\subsection{Performance Metrics}\label{metrics}
To help us understand what extra benefit we can achieve from new information in the cooperative model, we define \emph{three} metrics for the performance of video streaming given a known network. These metrics have been extensively studies in various approaches and here we use them as a benchmark to test our new model\cite{Qava, Avis,VideoMeasurement, Festive}.

\begin{enumerate}
\item\textit{Efficiency}: DASH based adaptation mechanisms have several different qualities (bit rates) for a single video. Since the video download is chunk-based, a video session can consist of chunks with different qualities. The video player will stitch consecutive chunks together, and the chunks can have different bit rates. The video player should download the chunk with the highest possible quality based on available bandwidth at each download point to give user better quality experience. To measure the efficiency of a video session, we use ratio between the amount of bandwidth used for video downloading and the maximum available throughput: $\bar{E}=\frac{\sum\limits_{t=1}^T D^t}{\sum\limits_{t=1}^T C^t}$, where $D^t$ stands for download rate at time t and $C^t$ stands for the maximum bandwidth for the user, higher $\bar{E}$ indicates better quality. Numerically $\sum\limits_{t=1}^T D^t = \sum\limits_{t=1}^T  \sum \limits_{i=1}^M R_i * x_i^t $ if not considering retransmission.

\item\textit{Stability}: From viewer's perspective, cons25tant switches between different qualities are distracting and not desirable. To assess the stability of video streaming, we use a a25verage fluctuation of bit-rate switches perceived by the viewers during a video session, and the fluctuation is the the number of jumps difference bewteen two consecutive sessions, \emph{fluctuation cost} at time t: $\phi^t =  \|i*x_i^t - i*x_i^{t-1}\|$, and the average jump frequency $\bar{\phi} =(\sum\limits_{t=1}^{T-1} \phi^t)/(T*INTV) $ over the entire play session (playtime = number of time segments $T$ * each time segment interval $INTV$), the lower the $\bar{\phi}$ the better.

\item\textit{Interruption}: Video play should not be halted for fetching video chunks, otherwise user experience will be severely degraded. Video play interruptions affect the user engagement and results in early abandonment of the video play. Video streaming players usually buffer extra chunks to absorb any sudden drop on the link capacity. We use the ratio of buffer time (the time halted for video downloading) over play time as an indication for interruption, lower ratio value indicates better streaming performance; $\beta = \frac{BufferTime}{PlayTime}$.



\end{enumerate}

\subsection{Architecture}\label{architecture}
\textbf{\emph{archi Figure goes here!}}
 
\begin{figure}[hb]
 \includegraphics[width=\linewidth]{Illu.jpg}
 \centering
 \caption{Illustration of Video Buffer}
\end{figure}
 

 
 
 
\section{Video optimization algorithms}
In this section we first introduce an offline optimization formulation, in which the application knows the available bandwidth for the whole video play session. It is called offline because the scheduler decides everything beforehand and it does not consider early abandonment of the video session. Then we come back to online algorithm where the user only knows the available bandwidth for the next t seconds. 
 
 
\subsection{Offline Optimization Formulation}
We first formulate the optimization problem in an offline manner, i.e., if we know the available bandwidth from time segment t=1 to T (each segment is usually 2-10 seconds long), how to schedule which to download. 

Our design prioritizes interruption avoidance and uses it as hard constraints, in other words at each time segment the download should always at least be equal to video played (assume at the initial state the play buffer is one chunk ahead of playing), i.e., $\sum \limits_{t=1}^\tau R_i * x_i^t\leq\sum\limits_{t=1}^\tau C^t$ for $\forall \tau \in \{1, \dots, T\}$. The optimization problem asks for the best chunk selection for each time segment t that maximizes the performance objective.

% While ensuring interruption-free, the video player should not buffer too many video chunks to avoid excessive waste of download if the early abandonment occurs. Here we set this as a parameter $z$, and we can capture this into constraint as well:  $\sum \limits_{t=1}^{\tau+z} R_i* x_i^t\geq\sum\limits_{t=1}^\tau C^t$ for $\forall \tau \in \{1, \dots, T-z\}$.

\textit{Performance Objective}: we define the utility as a function of efficiency and stability, $U = \bar{E} - \alpha * \bar{\phi} $, where $\bar{E}$ represents the average efficieny and $\bar{\phi}$ represents the instability, both are taken from definition in section \ref{metrics}. $\alpha$ here is a tunable parameter to tradeoff between efficiency and instability.

\textbf{Problem 1}: 

Max:\space \space $ \bar{E} - \alpha * \bar{\phi} $
%$\frac{\sum\limits_{t=1}^T  \sum \limits_{i=1}^M R_i * x_i^t }{\sum\limits_{t=1}^T C^t} - \alpha * \frac{\sum\limits_{t=2}^{T}  \| \sum \limits_{i=1}^M (i *x_i^t - i *x_i^{t-1})\| }{T*INTV}  $

Subject To:

$\forall \tau \in \{1, \dots, T\}; \sum \limits_{t=1}^\tau  \sum \limits_{i=1}^M R_i *x_i^t\leq\sum\limits_{t=1}^\tau C^t$

$\forall t, \forall i; \sum \limits_{i=1}^M x_i^t =1\; and\;\; x_i^t = \{0,1\}$

This is a very standard mix integer programming problem (MIP), and some well-developed MIP tools like IBM ILOG CPLEX can solve this type of problem even for a three hour long video (meaning there are thousands of variables) within a few seconds. This optimization problem only tells us each chunk with which play bit rate to download. In terms of when to download, we constantly saturate the link, and the download requests consist of a queue of video chunks with certain rates. 


\subsection{Continuous Online Algorithm} 
\begin{comment}
In the online algorithm, we take care of two relaxed assumptions in the offline algorithm: (i) it is hard to have a foresight of the available bandwidth for the whole video session; (ii) we cannot have unlimited chunk prefetching in case of video early abandonment. We assume we can forcast next $H$ time segment and $H$ is often small in practice; e.g., if $H=3$, the prediction already covers tens of seconds. 
%Here we allow a variable buffer window size z, which means we can hold minimum 1 chunk and maximum z chunks ahead of play progress. 

Since the number of the time segments is $z$ and current unfilled window size is $W$, so we need a planning for $H+z-(z-W)=W+H$ chunks download, which is the upperbound of chunks to be downloaded in H time segments. The continuous algorithm functions as the following: at each time segment $t$, it computes the local optimal schedule for downloading $L\leq W+H$ chunks in $H$ time segments and such that it maximizes the objective function within certain constraints, and we pick the optimal schedule and use its scheduled action at $t$. We iteratively compute the best schedule for time  $s=t+1, \dots$ at each time segment $s$ and keep udpating the window size continuously. The idea falls in the same class of Viterbi algorithm in Markov model.
\end{comment}
In this section we show that our exposed KPIs can be easily integrated into three existing end-to-end based video streaming algorithms, FESTIVE, BBA, and QDASH. Developed by several different research institutions and video streaming vendors, these algorithms represent a wide range of probing then deciding algorithms.\cite{QDASH, Festive} We made a small amount of change in those algorithms to absorb the extra KPI to help them make decisions, we present them as FESTIVE+, BBA+ and QDASH+. On average our change is less than 80 lines of code, comparing to 300 to 500 plus lines of code in these algorithms. 
\subsubsection{BBA+}
BBAs(Buffer-Based Algorithms) is a class of algorithms developed and used by Netflix. The idea is that given the complicated interactions between different layers, at video play steady state we can use the occupancy of video buffered in the streaming player as an indicator for video bit rate selection. BBAs assumee the video download is continous. We use the best algorithm BBA(2) as a base model. BBA(2) has two parts:
\begin{itemize}
 \item Steady state: the bit rate selection is a piecewise linear function of the buffer occupancy(seconds of video ahead of playback time), it stays at downloading minimum bit rate if the buffer occupancy in reservior range, grows linearly at cushion range maximum bit, and reaches maximum if beyond cushion range until it reaches maximum buffer, where it stops downloading. The reservior size is function of effective video encoding rate which fluctuates around the average encoding rate.
 
 \item Startup state: everytime the video starts, the buffer starts from empty, if we use the same function from steady state, it will stay at minimum bit rate until the buffer grows more than reservior. To address this problem, BBA(2) makes a fast start algorithm based on the pace of video buffer growth. 
\end{itemize}
\emph{Our changes:} BBA(2) algorithm takes more than 30 seconds to ramp up to stable bit rate, so we decide to change the start up phase to make it to reach stable bit rate faster. We also adjust both reservior and cushion size, and thus the slope for bit rate selection based on historical and KPIs. 

\begin{algorithm}[h]
\SetAlgoLined
 \KwData{ $C$: Link capacity for next 4 seconds\newline $B_{now}$: Current Buffer Occupancy\newline$V$: Reservior Size\newline $\Delta B$: buffer occupancy change in last time segment}
 \KwResult{$Rate$: Rate for next chunk }
% \BlankLine
\uIf{$B_{now}>V$}{
$Rate$ = BBA\_steady()} 
\Else{
$Rate_{infer} = \min\{Rate_{max}, 0.8*C \}$\;
\emph{if the buffer is filled at a certain pace, pick the inferred rate}\;
\uIf{$\Delta B >0.3(1-\frac{B_{now}}{V})*V$}{
$Rate = Rate_{infer}$
} \Else {\emph{Othewise, pick one level below}
$Rate = \max\{Rate_{min}, Rate_{infer-1}\}$
}
}
Update(Cushion Size, Reservior Size)\;
\caption{BBA(2) PLUS Rate Selection}
\end{algorithm}

\begin{algorithm}[h]
\SetAlgoLined
 \KwData{ $C$: Link capacity for next 4 seconds\newline $b_{ref}$: Computed bit rate\newline$b_{cur}$: Current play bit rate\newline$BitChangeTime$: last time bit rate change }
 \KwResult{$Rate$: Rate foralso next chunk}
 //Pick the bit rate right below link capacity, 0.9 is a factor for encoding overhead.
$b_{ref}= 0.9*\lfloor C \rfloor$\;

\uIf{$TimeNow-BitChangeTime>20$}{
\uIf{$b_{ref}\not = b_{cur}$}
{
$BitChangeTime = TimeNow$
}
\Return{$b_{ref}$}
} \Else{
$ score_{ref} = score_{e}(b_{ref}) +\alpha*score_{s}(b_{ref})$\;
$score_{cur} = score_{e}(b_{cur}) +\alpha*score_{s}(b_{cur})$
\uIf{$score_{ref} > score_{cur}$}{
$BitChangeTime = TimeNow$\;
\Return{$b_{ref}$}
} \Else {\Return{$b_{cur}$}}
}
\caption{FESTIVE PLUS Rate Selection}
\end{algorithm}

\subsubsection{FESTIVE+}
FESTIVE(Fairness, Efficient and Stable adapTIVE algorithm) algorithm\cite{Festive} assumes the following: there is a single bottleneck link in the network, which would be the wireless link in the cellular network, and the video downloading is very periodic, i.e., it has a noticeable on-off phase in terms of video chunk downloading. It has three components: 
\begin{enumerate}
 \item Select when the next chunk will be downloaded: it uses randomization of download start time ($t_{i}^{start}$) to avoid potential synchronization of congestion in the bottleneck link and increases the bandwidth estimation confidence. 
 \[
   t_{i+1}^{start}= 
\begin{cases}
    t_{i}^{end}, \; \;&\text{if}\;\; buffer_i < randbuf_i; \\
    t_{i}^{end}+ &buffer_i-randbuf_i,\;\; \text{  otherwise}
\end{cases}
\]

 \item Select a suitable bitrate for the next chunk: it uses delayed updates-if there has no play bit rate change in the last 20s, the streaming is free to change bit rates based on its bandwidth estimation. If an bit rate change is needed within 20 seconds, it has an efficiency cost $score_{e}{(b)}=|\frac{b}{\min{(w, b_{ref})}} -1|$, and a stability cost $score_{s}(b)=\begin{cases}
   2^n+1, \; \;&\text{if}\;\; b=b_{ref}; \\
     2^n, \; \;&\text{if}\;\; b=b_{cur};\\
\end{cases}$, and the combined score is linked by a tunable parameter $\alpha$, $b=argmin (score_{e}(b) +\alpha*score_{s}(b))$.
 
 \item Estimate the network bandwidth: it takes harmonic mean over the last 20s as an estimation for future bandwidth, since it is a more stable and conservative estimation.
 
\end{enumerate}
\emph{Our changes:} we make only one small change in their algorithm; (i) we replace harmonic mean with our KPI, which could be intepreted as a ground truth or very close estimation;(ii) we turn off the randomization since FESTIVE uses it to avoid inaccurate estimation while our model does not suffer this problem. We name our algorithm FESTIVE PLUS.




 
 
 




\section{Evaluation}

\subsection{Emulation}
We first briefly explain the experiments' setups and then show the results. 
\subsubsection{Setup}
We build our own emulation testbed to evaluate the performance gain from in-network knowledge. We have a server which gets video play bit rate requests, and sends video chunks with the requested play bit rate to the clients. We take 10 different video encoding rates from Netflix: $\{0.235, \dots$ $, 4.3\} $ Mbps. Since the requested rates are average bit rates and the chunk bit rates may vary based on whether the scene is static, we add randomization factor (0.2-2) for the video chunks and a weak correlation between two conservative chunks ($rate_{i+1} = rate_{i}*0.4 + 0.6* randomization\;factor$), the similar approximations are also used in previous works.\emph{BBA citation} The clients are running various video streaming algorithms such as BBA2, FESTIVE and QDASH. The server and the clients are both connected to a dummynet\cite{Dummynet}, which acts like an eNobeB and enforces scheduling or rate limiting. To expose the in-network knowledge, we build a different channel between dummynet (eNodeB) and the client. In fact an alternative approach is to convey the knowledge to the server and let the server decide.   
\begin{figure}[hb]
 \includegraphics[width=\linewidth]{architecture.jpg}
 \centering
 \caption{Illustration of Video Buffer}
\end{figure}

\emph{Trace Selection}: we choose to use both LTE and 3G traces from both MIT experiments \cite{Sprout} and AT\&T lab. For MIT traces, it has all major carriers and for business reason, we anonymize all traces and name them as LTE-a, LTE-b, LTE-c and 3G-a. \emph{Essentially they are Verizon LTE, MIT ATT4g, and ATT lab 4g, and sprint 3g}. 

\subsubsection{Results}


\subsection{Simulation}
\textit{Setup:} We take 10 different video encoding rates from Netflix: $\{0.235,\dots, 4.3\}$ Mbps, and we run the offline algorithm from a simulation trace with 105 users using real trace radio information. We use IBM ILOG CPLEX Optimizer to solve the mixed integer programming in ubuntu 14.04 with linux kernel 3.13.0-27-generic. The offline algorithm allows cross boundry downloading, i.e., we can download one chunk in two time segments as far as we have residual bandwidth, and 
we do not consider overhead in encoding rate and retransmission; in practice, video with 1 Mbps encoding rate usually consumes more than 1 Mbps bandwidth. The results from this offline algorithm can be used as a theoretical maximum for comparison, and help us understand the dynamics between the parameters in the system. Note the offline optimization prioritizes later chunks due to the waterfilling mechanism: it always uses residual bandwidth to download future chunks, so the future chunks can utilize both current and future bandwidth and thus have higher quality. 

\textit{Results:} 

Stability factor $\alpha$: we test different $\alpha$ values to see how the factor affects the dynamics between stability and efficiency. Through experiments we find $\alpha$ should be in a [1:5] range, any $\alpha>5$ leads to the condition where the video is stuck with low quality. 

$INTV$ for each segment: we test three different time segments \{2,5,10\} seconds, and find that the longer interval leads to better stability under the same $\alpha$ but the difference is trivial somehow. 

The number of prefetched chunks: we also find that for UEs with a reasonably stable maximum bandwidth (102/105 UEs), it requires at most prefetching chunks with a total playtime 20 seconds throughout the whole play, this ``magic" value is coincidentally used in Festive.   

Comparison between Festive and our offline approach: we use two parameters a=12 (stability index) and $p=0.85*HarmonicMean$ (0.85 is the bandwidth estimation discount factor), the two values taken from Festive. Since in Festive it assumes time interval is 2 seconds, so we also use 2 seconds interval per chunk in our evaluation. Festive can achieve an utilization of 64.3\%, and even if we take aggressive approach with no estimation bandwidth discount, i.e., $p=HarmonicMean$ still we can only get 68.7\% utilization. The reason is that Festive uses the historical harmonic mean as bandwidth estimation, which by nature underestimates the available bandwidth. In Festive for stability, the number of switches for quality is 0 with 0.85 discount factor, but constantly stuck with low quality; or 2 with no bandwidth discount. On the contrary even if we take a fair factor in stability, in this case, $\alpha=5$, our approach can still get 89.5\% utilization, while there is only one jump in the video quality.   

Based on the above observations and the reality, we decide to set $\alpha=5$ and $INTV=10s$ (for netflix, the time interval for each segment is 2 seconds for streaming through wireline and 10 seconds through wireless). and $z= \frac{20}{INTV} = 2$.



 \begin{figure} [hb]
 \includegraphics[width=\linewidth]{Festive.png}
 \centering
 \caption{Comparison between Festive and our offline approach for UE13}
\end{figure}

\section{Discussions}

\subsection{Security Concerns and User Cheating}
1. Privacy: User identity: How do CDN and streaming companies identify the user?

2. Business Relation: Is it safe to expose ISP's KPIs to cloud service provider?

3. Is it incentive compatible? In other words, can user cheat by requesting less or more bandwidth based on KPIs exposed to them? 

\subsection{Fairness against Non-cooperative Service}

\section{related work}


Approaches like AVIS \cite{Avis} sits at the gateway and have its own video chunk download scheduling system. The scheduler strikes a balance between fairness, efficiency and stability among users in the same cell. However in-network approaches lack the insight of the video play progress, e.g., video buffer occupancy. In addition, AVIS specifically focuses on the fairness of bandwidth not OFDMA resource blocks, and only prefetches one video chunk at each time. This may lead to starvation of users with good link quality and/or may have interruptions if the available bandwidth drops below minimum play bit rate. 
 
 
\bibliographystyle{acm}
\bibliography{references} 

\end{document}